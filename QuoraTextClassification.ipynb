{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_train = pd.read_csv('QuoraQuestions/train.csv')\n",
    "original_test = pd.read_csv('QuoraQuestions/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(original_train[original_train['target'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train[original_train['target'] == 1].head()['question_text'][30] # definitively insincere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train[original_train['target'] == 1].head()['question_text'][110] # on the cusp - could be considered insincere bc \"blacks\" is not necessarily PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train[original_train['target'] == 1].head()['question_text'][114] # lascivious and this is borderline incest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train[original_train['target'] == 1].head()['question_text'][115] # definitely insincere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train[original_train['target'] == 1].tail()['question_text'][1306093] # part 2 - incest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train[original_train['target'] == 1].tail()['question_text'][1306099] # racist towards pakistani people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train[original_train['target'] == 1].tail()['question_text'][1306094] # provocative and trying to make a statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_train1 = list(map(lambda x : x.lower().split(), original_train[original_train['target'] == 1]['question_text']))\n",
    "split_train0 = list(map(lambda x : x.lower().split(), original_train[original_train['target'] == 0]['question_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportion of Different Question Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# determine question type counts among sincere and sincere questions\n",
    "search1 = 'why'\n",
    "search2 = 'how'\n",
    "search3 = 'what'\n",
    "search4 = 'do'\n",
    "insincere_questions = [[] for _ in range(4)]\n",
    "sincere_questions = [[] for _ in range(4)]\n",
    "for sublist in split_train1:\n",
    "    if sublist[0] == search1:\n",
    "        insincere_questions[0].append(sublist)\n",
    "    elif sublist[0] == search2:\n",
    "        insincere_questions[1].append(sublist)\n",
    "    elif sublist[0] == search3:\n",
    "        insincere_questions[2].append(sublist)\n",
    "    elif sublist[0] == search4:\n",
    "        insincere_questions[3].append(sublist)\n",
    "for sublist in split_train0:\n",
    "    if sublist[0] == search1:\n",
    "        sincere_questions[0].append(sublist)\n",
    "    elif sublist[0] == search2:\n",
    "        sincere_questions[1].append(sublist)\n",
    "    elif sublist[0] == search3:\n",
    "        sincere_questions[2].append(sublist)\n",
    "    elif sublist[0] == search4:\n",
    "        sincere_questions[3].append(sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "insincere_question_prop = [len(insincere_questions[0])/num_insincere, len(insincere_questions[1])/num_insincere, len(insincere_questions[2])/num_insincere, len(insincere_questions[3])/num_insincere]\n",
    "sincere_question_prop = [len(sincere_questions[0])/num_sincere, len(sincere_questions[1])/num_sincere, len(sincere_questions[2])/num_sincere, len(sincere_questions[3])/num_sincere]\n",
    "question_names = [search1, search2, search3, search4]\n",
    "print question_names\n",
    "print insincere_question_prop\n",
    "print sincere_question_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Question Proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.barh(question_names, insincere_question_prop, color = 'teal')\n",
    "plt.title('Proportion of Question Types Among Insincere Qs', fontsize = 15)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 13)\n",
    "plt.xlabel('Density', fontsize = 13)\n",
    "plt.ylabel('Question', fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.barh(question_names, sincere_question_prop, color = 'orange')\n",
    "plt.title('Proportion of Question Types Among Sincere Qs', fontsize = 15)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 13)\n",
    "plt.xlabel('Density', fontsize = 13)\n",
    "plt.ylabel('Question', fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment: \n",
    "People who ask insincere questions use significantly more \"why\" questions than any other question type. The \"why\" question assumes a statement to be true, which can be problematic in some cases. For many insincere \"why\" questions, the underlying assumption is inconclusive or is subjective, so the individual is more likely to be asserting an opinion rather than genuinely seeking the answer to a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use this to determine common n-grams\n",
    "#val = all(x in sublist for x in [search1]) # use this to find a combination of words in a sentence\n",
    "from nltk.util import ngrams\n",
    "search1 = 'jews'\n",
    "search2 = 'and'\n",
    "search3 = 'blacks'\n",
    "questions = []\n",
    "for sublist in split_train1:\n",
    "    n_gram = list(ngrams(sublist, 3)) \n",
    "    ng = (search1, search2, search3)\n",
    "    if ng in n_gram:\n",
    "        questions.append(sublist)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Common Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "concatenated_split_train1 = list(itertools.chain.from_iterable(split_train1))\n",
    "concatenated_split_train0 = list(itertools.chain.from_iterable(split_train0))\n",
    "concatenated_split_train1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "s = stopwords.words('english')\n",
    "concatenated_split_train1 = list(filter(lambda x : x not in s, concatenated_split_train1))  # filter out all stop words (e.g. pronouns, articles)\n",
    "concatenated_split_train0 = list(filter(lambda x : x not in s, concatenated_split_train0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "top = pd.Series(concatenated_split_train1).value_counts()[:40] # top 40 words \n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title(\"Top 40 Most Frequent Words - Insincere Questions\", fontsize = 20)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel('Word', fontsize = 17)\n",
    "plt.xlabel('Count', fontsize = 17)\n",
    "top.plot(kind='barh', color = 'teal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#top = pd.Series(concatenated_split_train0).value_counts()[:40] # top 40 words \n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title(\"Top 40 Most Frequent Words - Sincere Questions\", fontsize = 20)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel('Word', fontsize = 17)\n",
    "plt.xlabel('Count', fontsize = 17)\n",
    "top.plot(kind='barh', color = 'orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Features for Training/Testing Data\n",
    "Our input values will be constructed as follows:\n",
    "* Features = columns\n",
    "* Samples = rows\n",
    "\n",
    "Let's concatenate the questions from the training data and testing data so that we can create the features based on the entire dataset. If we were to do this for the training and testing datasets individually, this would cause errors when we try to predict values from our testing dataset due to different number of dimensions in the number of columns in train compared to test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate train and test\n",
    "import copy\n",
    "X = copy.deepcopy(original_train['question_text'])\n",
    "Z = copy.deepcopy(original_test['question_text'])\n",
    "X = X.append(Z)\n",
    "X= X.reset_index(drop = True)\n",
    "print X.shape # number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(Z) # length of test dataset\n",
    "print float(len(Z))/len(X) # proportion of the entire dataset that is testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print float(len(X) - len(Z))/len(X) # proportion of the dataset that is training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's split our training dataset into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='datasplit.png')\n",
    "# this is how our data will be split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1230000\n",
    "print float(train_size)/len(X) # first 90% will be train data\n",
    "\n",
    "valid_size = len(X) - len(Z)\n",
    "print float(valid_size)/len(X) -  float(train_size)/len(X) # the remaining ~6% will be validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, create features through word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gets word counts of all unique words in the dataset - Bag of Words Representation (order doesn't matter)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(stop_words = 'english') # remove stop_words (e.g. the, a, in, pronouns, etc.)\n",
    "X_train_counts = count_vect.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts word counts into word frequencies with values between 0 and 1 - this also normalizes the data\n",
    "# word frequencies are calculated by quora question not frequency across the entire dataset\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape # the shape here is the (num_samples, num_features) where num_features == unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0] # initial format of question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf[0] # 1 sample - the question converted into a vector of counts and then a vector of word frequencies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "scorer = ['accuracy', 'precision', 'recall', 'f1']\n",
    "algorithm = BernoulliNB()\n",
    "num_splits = 5\n",
    "cv_results = cross_validate(algorithm, X_train_tfidf[:len(X) - len(Z)], original_train['target'], scoring = ('accuracy', 'precision', 'recall', 'f1'), cv = num_splits, return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_splits = 5\n",
    "print \"Accuracy: \" + str(sum(cv_results['test_accuracy'])/num_splits)\n",
    "print \"Precision: \" + str(sum(cv_results['test_precision'])/num_splits)\n",
    "print \"Recall: \" + str(sum(cv_results['test_recall'])/num_splits)\n",
    "print \"Harmonic Mean: \" + str(sum(cv_results['test_f1'])/num_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hihi! Don't worry about anything below here - I'm still making changes:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Model \n",
    "#### Bernoulli Naive Bayes Classifier \n",
    "Note: I used a BernoulliNB Classifier since other classifiers have a much longer run time or have poorer performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function displays the metrics for evaluation of the model.\n",
    "from sklearn import metrics\n",
    "def results(expected, actual):\n",
    "    not_equal = actual[actual != expected]\n",
    "    fn = not_equal[not_equal == 0] # false negatives\n",
    "    accuracy = metrics.accuracy_score(expected, actual)\n",
    "    recall = metrics.recall_score(expected, actual)\n",
    "    precision = metrics.precision_score(expected, actual)\n",
    "    harmonic_mean = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print 'Accuracy Score: ' + str(accuracy) # accuracy score based on our validation data \n",
    "    print 'Recall Score: ' + str(recall)\n",
    "    print 'Precision Score: ' + str(precision)\n",
    "    print 'Harmonic Mean: ' + str(harmonic_mean) + '\\n'\n",
    "    print 'Expected Insincere: ' + str(len(expected[expected == 1])) # insincere\n",
    "    print 'Actual Number Insincere: ' + str(len(actual[actual == 1])) # insincere\n",
    "    print 'Number of False Negatives: ' + str(len(fn))\n",
    "    print 'Actual Number Sincere: ' + str(len(actual[actual == 0])) # sincere\n",
    "    print 'Total: ' + str(len(expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function fits the data to a model and yields fitted values from the model\n",
    "def model(alg, x, y, training_size, validation_size):\n",
    "    classifier = alg.fit(x[:training_size], y[:training_size]) \n",
    "    validation = classifier.predict(x[training_size:validation_size]) \n",
    "    predicted = classifier.predict(X_train_tfidf[validation_size:]) # no target data available\n",
    "    results(original_train['target'][training_size:validation_size], validation)\n",
    "    return validation, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function displays some of the sample questions and their predicted target values\n",
    "import numpy as np\n",
    "def display_samples(question_type, x_data, v_size, pred):\n",
    "    if question_type:\n",
    "        indices = np.where(pred == 1)\n",
    "    else: \n",
    "        indices = np.where(pred == 0)\n",
    "    print 'Here are a few samples with their target values.'\n",
    "    count = 0\n",
    "    for i in list(indices[0]):\n",
    "        if count > 10: # display 10 samples\n",
    "            break\n",
    "        print x_data[v_size + i]\n",
    "        print pred[i] \n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model \n",
    "For the base model, let's set all target values to 0, where 0 is the class for sincere questions, since the majority of questions are sincere. You'll see that the number of insincere predicted values is 0. This is because we didn't train on any insincere values. Also, the recall and precision scores are all 0. This is because we did not train on any data where the target value == 1, so it's not possible to have a true positive (value == 1), which is the numerator of the recall and precision scores. \n",
    "* recall = TP / TP + FN\n",
    "* precision = TP / TP + FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "algorithm = BernoulliNB()\n",
    "y_base = list(copy.deepcopy(original_train['target'][:train_size]))\n",
    "y_base = list(map(lambda x: x*0, y_base))\n",
    "y_valid, y_pred = model(algorithm, X_train_tfidf, y_base, train_size, valid_size)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model With Observed Data\n",
    "Now, let's see how the model performs on our observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "algorithm = BernoulliNB()\n",
    "y_valid, y_pred = model(algorithm, X_train_tfidf, original_train['target'], train_size, valid_size) \n",
    "# we want all of the proportions below to be as close to 1 as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function displays questions which were predicted to be sincere but Quora deems them to be insincere \n",
    "def display_false_negatives(expected, actual, input_questions, index):\n",
    "    fn_indices = list(np.where((actual != expected) & (actual == 0))[0])\n",
    "    print np.take(list(input_questions), fn_indices)[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_false_negatives(original_train['target'][train_size:valid_size], y_valid, X[train_size:valid_size], index = 0)\n",
    "# change the index to see different samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Results\n",
    "Let's see some of our samples from the testing data (includes input and output values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display insincere samples\n",
    "display_samples(1, X, valid_size, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display sincere samples\n",
    "display_samples(0, X, valid_size, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To improve performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: cross validate data by using training and testing on differnet chunks of the data\n",
    "* example: x = [1, 2, 3, 4, 5, 6, 7, 8, 9 , 10]\n",
    "    define train size = 2\n",
    "    run the model on  the following splits and compoare the \n",
    "    * train = x[0:8]; validation = x[8:10]\n",
    "    * train = x[1:9]; validation = [x[0], x[9]]\n",
    "    * train = x[2:10], validation = [x[0:2]]\n",
    "    * validation = [x[0], x[8]]; train = the rest\n",
    "    * validation = [x[1], x[4]]; train = the rest\n",
    "    * finish for all combinations\n",
    "\n",
    "The final accuracy = the average of the accuracies from each split.\n",
    "It's the same for recall and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: create features with n-grams (instead of using words as feature columns, use phrases with n words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: try different models if the runtime isn't too long\n",
    "# try SVM, it generally has good performance for most classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To explore and visualize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bigram & trigram plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implement ROC curve for each of our cross validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how many inscinere/sincere questions has a particular word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# which words produce the highest precision? recall? as in search for all the sentences with 'blacks' - call them insincere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: group questions into themes: e.g. racism, sexism, homophobic, sexuality, religious intolerance, \n",
    "# prejudice towards immigrants, unproductive political criticism\n",
    "# Visualize this data\n",
    "# Show percentages of each type\n",
    "\n",
    "# what we define these themes to be: \n",
    "    # look for all insincere questions that include a word denoting an ethnic group (e.g. blacks, jews, muslims)\n",
    "    # look for all insincere questions that include a word denoting a sex(e.g. woman, women, man, men)\n",
    "    # look for all insincere questions that include a word denoting a sexual preference(e.g gay, homo, fag, trans, transgender, LGBTQ)\n",
    "    # look for all insincere questions that include a word denoting a religious group*( e.g. religious people, jews, muslims, christians)\n",
    "    # look for all insincere questions that include a word denoting a sexual term (sexy, gay, kiss, etc.)\n",
    "    # look for all insincere questions that include a word denoting immigrants (immigrants, fabs, fresh off the boat)\n",
    "    # look for all insincere questions that include words(s) or phrases(s) denoting a politcal topic (liberals, libs, libtards, democrats, republicans, conservatives, nazis, neo-nazis, president's name, politcal commentators, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# larger question for discussion section: although people make such discriminatory remarks, should we censor the online public?\n",
    "# what are the implications of online censorship? \n",
    "    # Argument against censorship: people have dark, deep thoughts that may be contentious but perhaps they truly believe \n",
    "    # those thoughts and are genuinely curious. The online public could offer a medium to express and discuss those thoughts. \n",
    "    # One key advantage of online forums is the option to be anonymous, which makes one more open to share dark ideas. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
